{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "from torch import cuda\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from reports import files, report_list\n",
    "from langchain.docstore.document import Document\n",
    "import configparser\n",
    "import logging\n",
    "\n",
    "\n",
    "# read all the necessary variables\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "path_to_data = \"../../reports/\"       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------------------fucntions -------------------------------------------##\n",
    "def getconfig(configfile_path:str):\n",
    "    \"\"\"\n",
    "    configfile_path: file path of .cfg file\n",
    "    \"\"\"\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    try:\n",
    "        config.read_file(open(configfile_path))\n",
    "        return config\n",
    "    except:\n",
    "        logging.warning(\"config file not found\")\n",
    "        \n",
    "def open_file(filepath):\n",
    "    with open(filepath) as file:\n",
    "        simple_json = json.load(file)\n",
    "    return simple_json\n",
    "\n",
    "def load_chunks():\n",
    "    \"\"\"\n",
    "    this method reads through the files and report_list to create the vector database\n",
    "    \"\"\"\n",
    "\n",
    "    #  we iterate through the files which contain information about its\n",
    "    # 'source'=='category', 'subtype', these are used in UI for document selection\n",
    "    #  which will be used later for filtering database\n",
    "    config = getconfig(\"../../model_params.cfg\")\n",
    "    all_documents = {}\n",
    "    categories = list(files.keys())\n",
    "    # iterate through 'source'\n",
    "    for category in categories:\n",
    "        print(\"documents splitting in source:\",category)\n",
    "        all_documents[category] = []\n",
    "        subtypes = list(files[category].keys())\n",
    "        # iterate through 'subtype' within the source\n",
    "        # example source/category == 'District', has subtypes which is district names\n",
    "        for subtype in subtypes:\n",
    "            print(\"document splitting for subtype:\",subtype)\n",
    "            for file in files[category][subtype]:\n",
    "\n",
    "                # load the chunks\n",
    "                try:\n",
    "                    doc_processed = open_file(path_to_data + file + \"/\"+ file+ \".chunks.json\" )\n",
    "\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Exception: \", e)\n",
    "                print(\"chunks in subtype:\",subtype, \"are:\",len(doc_processed))\n",
    "\n",
    "                # add metadata information \n",
    "                chunks_list = []\n",
    "                for doc in doc_processed:\n",
    "                    chunks_list.append(Document(page_content= doc['content'], \n",
    "                             metadata={\"source\": category,\n",
    "                                      \"subtype\":subtype,\n",
    "                                      \"year\":file[-4:],\n",
    "                                      \"filename\":file,\n",
    "                                      \"page\":doc['metadata']['page'],\n",
    "                                      \"headings\":doc['metadata']['headings']}))\n",
    "\n",
    "                all_documents[category].append(chunks_list)\n",
    "    \n",
    "    # convert list of list to flat list\n",
    "    for key, docs_processed in all_documents.items():\n",
    "        docs_processed = [item for sublist in docs_processed for item in sublist]\n",
    "        print(\"length of chunks in source:\",key, \"are:\",len(docs_processed))\n",
    "        all_documents[key] = docs_processed\n",
    "    all_documents['allreports'] = [sublist for key,sublist in all_documents.items()]\n",
    "    all_documents['allreports'] = [item for sublist in all_documents['allreports'] for item in sublist]\n",
    "    # define embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_kwargs = {'device': device},\n",
    "        encode_kwargs = {'normalize_embeddings': bool(int(config.get('retriever','NORMALIZE')))},\n",
    "        model_name=config.get('retriever','MODEL')\n",
    "    )\n",
    "    # placeholder for collection\n",
    "    qdrant_collections = {}\n",
    "    \n",
    "    \n",
    "    for file,value in all_documents.items():\n",
    "        if file == \"allreports\":\n",
    "            print(\"emebddings for:\",file)\n",
    "            qdrant_collections[file] = Qdrant.from_documents(\n",
    "                value,\n",
    "                embeddings,\n",
    "                path=\"../../qdrant/\",\n",
    "                #location=\":memory:\", \n",
    "                collection_name=\"uganda-documents\",\n",
    "            )\n",
    "    print(qdrant_collections)\n",
    "    print(\"vector embeddings done\")\n",
    "    return all_documents, qdrant_collections, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents splitting in source: Consolidated\n",
      "document splitting for subtype: Consolidated Annnual Report\n",
      "chunks in subtype: Consolidated Annnual Report are: 2382\n",
      "chunks in subtype: Consolidated Annnual Report are: 3485\n",
      "length of chunks in source: Consolidated are: 5867\n",
      "emebddings for: allreports\n",
      "{'allreports': <langchain_community.vectorstores.qdrant.Qdrant object at 0x1398415a0>}\n",
      "vector embeddings done\n"
     ]
    }
   ],
   "source": [
    "all_documents, vectorstores, embeddings = load_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.qdrant import Qdrant\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "config = getconfig(\"../../model_params.cfg\")\n",
    "\n",
    "vector_store = Qdrant.from_existing_collection(\n",
    "    path=\"../../qdrant/\", collection_name=\"uganda-documents\",\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_kwargs = {'device': device},\n",
    "        encode_kwargs = {'normalize_embeddings': bool(int(config.get('retriever','NORMALIZE')))},\n",
    "        model_name=config.get('retriever','MODEL')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
